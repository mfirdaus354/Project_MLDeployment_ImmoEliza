{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GETTING SUMMARY STATISTICS ON COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################### INITIALIZING CURRENT_DIR ################################################################################\n",
    "import os\n",
    "\n",
    "def CURRENT_DIR():\n",
    "    cwd = os.getcwd()\n",
    "    return os.chdir(cwd[:(cwd.index(\"Eliza\")+5)])\n",
    "\n",
    "CURRENT_DIR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################### MODULE USED IN THIS NOTEBOOK ################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from typing import Optional, Union, Literal\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV \n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "from src.config import Config, ModelConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['address', 'bedroom_count', 'district', 'epc_score', 'habitable_surface', 'immo_status', 'immocode', 'land_surface', 'municipality', 'plot_area', 'postalcode', 'price', 'price_per_sqmeter', 'province', 'region', 'room_count', 'subtype', 'type'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'correlation betwwe price and other variables'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "price                               1.00\n",
       "price_per_sqmeter                   0.17\n",
       "plot_area                           0.15\n",
       "habitable_surface                   0.34\n",
       "land_surface                        0.15\n",
       "bedroom_count                       0.38\n",
       "room_count                          0.38\n",
       "postalcode                         -0.12\n",
       "district                           -0.22\n",
       "province                            0.14\n",
       "region                              0.06\n",
       "Name: price, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####################################################################################### GETTING TO KNOW DF_MAIN ################################################################################\n",
    "\n",
    "# import libaries\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "\n",
    "# loading data_forsale_new.csv into df_main\n",
    "df_main = ModelConfig.load_data(\"data/data_forsale_new.csv\", \"csv\")\n",
    "\n",
    "# checking out on df_main\n",
    "Config.expand_display(df_main.columns)\n",
    "Config.expand_display(x=\"correlation betwwe price and other variables\", y=df_main[[\"price\", \"price_per_sqmeter\", \"plot_area\", \"habitable_surface\", \n",
    "         \"land_surface\", \"bedroom_count\", \"room_count\", 'postalcode', 'district', 'province', 'region']].corr()[\"price\"], )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the calculated correlation value above, we can pick 3 variables that have an adequately significant correlation with variable **price**:\n",
    "1. **habitable_surface** : 0.344623\n",
    "2. **room_count**        : 0.376763\n",
    "3. **bedroom_count**     : 0.382781\n",
    "\n",
    "But in the end, I will use these columns:\n",
    "Variable            | Target/Feature   | Correlation (Poisson)\n",
    "price               |                  |       1.00\n",
    "price_per_sqmeter   |                  |       0.17\n",
    "plot_area           |                  |       0.15\n",
    "habitable_surface   |                  |       0.34\n",
    "land_surface        |                  |       0.15\n",
    "bedroom_count       |                  |       0.38\n",
    "room_count          |                  |       0.38\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 1 - TREE BOOSTED, NONLINEAR REGRESSION POWERED BY XGBREGRESSOR\n",
    "\n",
    "MODEL 1 is the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'eval_metric'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m y_preprocessed \u001b[39m=\u001b[39m preprocessing_pipeline\u001b[39m.\u001b[39mfit_transform(y)\n\u001b[0;32m     34\u001b[0m \u001b[39m# MAKING MACHINE LEARNING MODEL\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m xgb_reg, X_train, X_test, y_train, y_test, y_pred, eval_results, results \u001b[39m=\u001b[39m ModelConfig\u001b[39m.\u001b[39;49mXGBREGRConfig(X_preprocessed , y_preprocessed)\n\u001b[0;32m     37\u001b[0m \u001b[39m# train_losses = eval_results['validation_0']['mphe']\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39m# test_losses = eval_results['validation_1']['mphe']\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39m# epochs = range(1, len(train_losses) + 1)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39m# mphe = xgb_reg.score(X_test, y_test)\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[39m# print(f\"This XGBoost Regression Model generates Mean Pseud0 Huber Error of {mphe}\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dimas\\Desktop\\BECODE\\PROJECTS\\Project_MLDeployment_ImmoEliza\\src\\config.py:287\u001b[0m, in \u001b[0;36mModelConfig.XGBREGRConfig\u001b[1;34m(x, y, test_size, random_state)\u001b[0m\n\u001b[0;32m    280\u001b[0m     steps_taken\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39mTrain-Test Split has been done\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    282\u001b[0m \u001b[39m# Fit the model to the training data and evaluate it on the testing data\u001b[39;00m\n\u001b[0;32m    283\u001b[0m xgb_reg\u001b[39m.\u001b[39mfit(\n\u001b[0;32m    284\u001b[0m     X_train,\n\u001b[0;32m    285\u001b[0m     y_train,\n\u001b[0;32m    286\u001b[0m     eval_set\u001b[39m=\u001b[39m[(X_train, y_train), (X_test, y_test)],\n\u001b[1;32m--> 287\u001b[0m     eval_metric\u001b[39m=\u001b[39mparams[\u001b[39m\"\u001b[39;49m\u001b[39meval_metric\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    288\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    289\u001b[0m )\n\u001b[0;32m    291\u001b[0m \u001b[39mif\u001b[39;00m xgb_reg:\n\u001b[0;32m    292\u001b[0m     steps_taken\u001b[39m.\u001b[39mappend(\n\u001b[0;32m    293\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe training datasets have been fitted to XGBRegressor. Evaluation datasets and evaluation metrics have been loaded.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    294\u001b[0m     )\n",
      "\u001b[1;31mKeyError\u001b[0m: 'eval_metric'"
     ]
    }
   ],
   "source": [
    "####################################################################################### MODEL-1 ########################################################################################################\n",
    "\n",
    "# Import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.config import Config, ModelConfig\n",
    "from sklearn.model_selection import  train_test_split, cross_val_score, KFold\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Loading data_forsale_new.csv into df_main\n",
    "df_set_one = ModelConfig.load_data(filepath='./data/data_forsale_new.csv', file_type=\"csv\", usecols=[\"price\", \"plot_area\", \"habitable_surface\", \"bedroom_count\", \"land_surface\", \"room_count\"])\n",
    "\n",
    "# Apply log transformation to the 'price' column\n",
    "df_set_one['price'] = np.log1p(df_set_one['price'])\n",
    "\n",
    "# Extract the features (X) and target (y)\n",
    "X, y = ModelConfig.feature_target_config(df=df_set_one)\n",
    "\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Create preprocessing_pipeline\n",
    "preprocessing_pipeline=ModelConfig.PimpMyPipeline(steps=['knn_imputer', 'poly_features', 'std_scaler'], poly_degree=5)\n",
    "\n",
    "# fit_transform X, y using preprocessing_pipeline\n",
    "X_preprocessed = preprocessing_pipeline.fit_transform(X)\n",
    "y_preprocessed = preprocessing_pipeline.fit_transform(y)\n",
    "\n",
    "# # MAKING MACHINE LEARNING MODEL\n",
    "# xgb_reg, X_train, X_test, y_train, y_test, y_pred, eval_results, results = ModelConfig.XGBREGRConfig(X_preprocessed , y_preprocessed)\n",
    "\n",
    "# train_losses = eval_results['validation_0']['mphe']\n",
    "# test_losses = eval_results['validation_1']['mphe']\n",
    "# epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "# # Perform cross-validation with custom Pseudo Huber loss scorer\n",
    "# cv = KFold(n_splits=7, shuffle=True, random_state=42)\n",
    "# neg_mse = cross_val_score(xgb_reg, X_test, y_pred, cv=cv, scoring='neg_mean_squared_error')\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plt.plot(epochs, train_losses, label='Pseudo-Huber Loss During Training')\n",
    "# plt.plot(epochs, test_losses, label='Pseudo-Huber Loss During Testing')\n",
    "# plt.xlabel('Number of Boosting Iterations')\n",
    "# plt.ylabel('Pseudo-Huber Loss')\n",
    "# plt.legend()\n",
    "# plt.title('Training and Validation Loss History (Pseudo-Huber Loss)')\n",
    "# plt.show()\n",
    "\n",
    "# # Scatter Plot of Predicted Values vs. Actual Values \n",
    "# actual_values = np.array(X_test)\n",
    "\n",
    "# # # Sort the test data points based on feature values\n",
    "# sort_indices = np.argsort(X_test[:, 0])\n",
    "# X_test = X_test[sort_indices]\n",
    "# y_test = y_test[sort_indices]\n",
    "# y_pred_sorted = y_pred[sort_indices]\n",
    "\n",
    "# # Visualize predictions\n",
    "# plt.scatter(X_test[:, 0], y_test, color=\"darkblue\", label=\"test data\")\n",
    "# plt.plot(X_test[:, 0], y_pred_sorted, color=\"red\", label=\"XGBoost regression line\")\n",
    "# plt.title(\"XGBoost Regression with Polynomial Features\")\n",
    "# plt.xlabel('True Target Values')\n",
    "# plt.ylabel('Predicted Target Values')\n",
    "# plt.xscale(\"linear\")\n",
    "# plt.yscale(\"linear\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Evaluation\n",
    "# mphe = xgb_reg.score(X_test, y_test)\n",
    "# print(f\"This XGBoost Regression Model generates Mean Pseud0 Huber Error of {mphe}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3234, 32) (12936, 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "plot_area  habitable_surface  bedroom_count  land_surface  room_count  price\n",
       "False      False              False          False         False       False    3234\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3234, 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import xgboost as XGB\n",
    "from sklearn.model_selection import  train_test_split, cross_val_score, KFold\n",
    "\n",
    "xgb_reg = XGB.XGBRFRegressor()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_preprocessed, y_preprocessed, test_size=0.20, random_state=12\n",
    "            )\n",
    "\n",
    "sort_indices = np.argsort(X_test[:, 0]) #template to begin sorting indices\n",
    "X_train_sorted = X_train[sort_indices] #sorting X_train\n",
    "X_test_sorted = X_test[sort_indices] #sorting x_test\n",
    "y_test_sorted = y_test[sort_indices] #sorting y_test\n",
    "y_train_sorted = y_train[sort_indices]# sorting y_train\n",
    "# y_pred_sorted = y_pred[sort_indices]\n",
    "\n",
    "X\n",
    "print(X_train_sorted.shape, X_train.shape)\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train_sorted[:, :5], columns=[\"plot_area\", \"habitable_surface\", \"bedroom_count\", \"land_surface\", \"room_count\"])\n",
    "X_test_df = pd.DataFrame(X_test_sorted[:, :5], columns=[\"plot_area\", \"habitable_surface\", \"bedroom_count\", \"land_surface\", \"room_count\"])\n",
    "y_train_df = pd.DataFrame(y_train_sorted[:, :1], columns=[\"price\"])\n",
    "y_test_df = pd.DataFrame(y_test_sorted[:, :1], columns=[\"price\"])\n",
    "\n",
    "dtrain = pd.concat(objs=[X_train_df, y_train_df], axis=1)\n",
    "# Config.expand_display(dtrain.head())\n",
    "Config.expand_display(x=dtrain.isna().value_counts())\n",
    "\n",
    "\n",
    "DTRAIN = XGB.DMatrix(data=dtrain)\n",
    "Config.expand_display(x=DTRAIN.get_weight())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Config.expand_display(X_train_df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
