{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GETTING SUMMARY STATISTICS ON COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dimas\\Desktop\\BECODE\\PROJECTS\\Project_MLDeployment_ImmoEliza\n",
      "c:\\Users\\dimas\\Desktop\\BECODE\\PROJECTS\\Project_MLDeployment_ImmoEliza\\model-building\n"
     ]
    }
   ],
   "source": [
    "####################################################################################### CONFIGURENT CURRENT WORKING DIRECTORY #################################################################################\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd[:(cwd.index(\"Eliza\")+5)])\n",
    "os.chdir(cwd[:(cwd.index(\"Eliza\")+5)])\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['address', 'bedroom_count', 'district', 'epc_score', 'habitable_surface', 'immo_status', 'immocode', 'land_surface', 'municipality', 'plot_area', 'postalcode', 'price', 'price_per_sqmeter', 'province', 'region', 'room_count', 'subtype', 'type'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "price                               1.00\n",
       "price_per_sqmeter                   0.17\n",
       "plot_area                           0.15\n",
       "habitable_surface                   0.34\n",
       "land_surface                        0.15\n",
       "bedroom_count                       0.38\n",
       "room_count                          0.38\n",
       "postalcode                         -0.12\n",
       "district                           -0.22\n",
       "province                            0.14\n",
       "region                              0.06\n",
       "Name: price, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####################################################################################### GETTING SUMMARY STATISTICS ################################################################################\n",
    "\n",
    "\n",
    "# import libaries\n",
    "import pandas as pd\n",
    "from src.config import Config, ModelConfig\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# loading data_forsale_new.csv into df_main\n",
    "df_main = ModelConfig.load_data(\"data/data_forsale_new.csv\", \"csv\")\n",
    "\n",
    "# checking out on df_main\n",
    "Config.expand_display(df_main.columns)\n",
    "Config.expand_display(x=df_main[[\"price\", \"price_per_sqmeter\", \"plot_area\", \"habitable_surface\", \n",
    "         \"land_surface\", \"bedroom_count\", \"room_count\", 'postalcode', 'district', 'province', 'region']].corr()[\"price\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the calculated correlation value above, we can pick 3 variables that have an adequately significant correlation with variable **price**:\n",
    "1. **habitable_surface** : 0.344623\n",
    "2. **room_count**  : 0.376763\n",
    "3. **bedroom_count** : 0.382781"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 1 - DECISION TREE REGULARISED REGRESSION \n",
    "\n",
    "MODEL 1 is the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m StandardScaler\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline\u001b[39;00m \u001b[39mimport\u001b[39;00m Pipeline\n\u001b[1;32m---> 13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexpress\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpx\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# Loading data_forsale_new.csv into df_main\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "####################################################################################### MODEL-1 ########################################################################################################\n",
    "\n",
    "# Import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.config import Config, ModelConfig\n",
    "from sklearn.model_selection import  train_test_split, cross_val_score, KFold\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Loading data_forsale_new.csv into df_main\n",
    "df_set_one = ModelConfig.load_data(filepath='./data/data_forsale_new.csv', file_type=\"csv\", usecols=[\"price\", \"plot_area\", \"habitable_surface\", \"bedroom_count\", \"land_surface\", \"room_count\"])\n",
    "\n",
    "# Apply log transformation to the 'price' column\n",
    "df_set_one['price'] = np.log1p(df_set_one['price'])\n",
    "\n",
    "# Extract the features (X) and target (y)\n",
    "X, y = ModelConfig.feature_target_config(df=df_set_one)\n",
    "\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Create preprocessing_pipeline\n",
    "preprocessing_pipeline=ModelConfig.PimpMyPipeline(steps=['knn_imputer', 'poly_features', 'std_scaler'], poly_degree=3)\n",
    "\n",
    "# fit_transform X, y using preprocessing_pipeline\n",
    "X_preprocessed = preprocessing_pipeline.fit_transform(X)\n",
    "y_preprocessed = preprocessing_pipeline.fit_transform(y)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                    X_preprocessed, \n",
    "                                    y_preprocessed, \n",
    "                                    test_size=0.20, \n",
    "                                    random_state=4521,\n",
    "                                    )\n",
    "\n",
    "\n",
    "# Initialize the XGBRegressor with specified, preconfigured parameters using ModelConfig\n",
    "xgb_reg = ModelConfig.XGBREGRConfig()\n",
    "\n",
    "xgb_reg.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)])\n",
    "\n",
    "y_pred = xgb_reg.predict(X_test)\n",
    "\n",
    "# Get the evaluation results from the XGBRegressor model\n",
    "eval_results = xgb_reg.evals_result()\n",
    "\n",
    "train_losses = eval_results['validation_0']['mphe']\n",
    "test_losses = eval_results['validation_1']['mphe']\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(epochs, train_losses, label='Pseudo-Huber Loss During Training')\n",
    "plt.plot(epochs, test_losses, label='Pseudo-Huber Loss During Testing')\n",
    "plt.xlabel('Number of Boosting Iterations')\n",
    "plt.ylabel('Pseudo-Huber Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss History (Pseudo-Huber Loss)')\n",
    "plt.show()\n",
    "\n",
    "# Scatter Plot of Predicted Values vs. Actual Values \n",
    "actual_values = np.array(X_test)\n",
    "#\n",
    "# # Perform cross-validation with custom Pseudo Huber loss scorer\n",
    "# cv = KFold(n_splits=7, shuffle=True, random_state=42)\n",
    "# neg_mse = cross_val_score(xgb_reg, X_test, y_pred, cv=cv, scoring='neg_mean_squared_error')\n",
    "\n",
    "\n",
    "# # # Sort the test data points based on feature values\n",
    "# sort_indices = np.argsort(X_test[:, 0])\n",
    "# X_test = X_test[sort_indices]\n",
    "# y_test = y_test[sort_indices]\n",
    "# y_pred_sorted = y_pred[sort_indices]\n",
    "\n",
    "# # Visualize predictions\n",
    "# plt.scatter(X_test[:, 0], y_test, color=\"darkblue\", label=\"test data\")\n",
    "# plt.plot(X_test[:, 0], y_pred_sorted, color=\"red\", label=\"XGBoost regression line\")\n",
    "# plt.title(\"XGBoost Regression with Polynomial Features\")\n",
    "# plt.xlabel('True Target Values')\n",
    "# plt.ylabel('Predicted Target Values')\n",
    "# plt.xscale(\"linear\")\n",
    "# plt.yscale(\"linear\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Evaluation\n",
    "# mphe = xgb_reg.score(X_test, y_test)\n",
    "# print(f\"This XGBoost Regression Model generates Mean Pseud0 Huber Error of {mphe}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
