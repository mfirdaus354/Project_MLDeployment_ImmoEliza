{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GETTING SUMMARY STATISTICS ON COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## GETTING SUMMARY STATISTICS ############################\n",
    "\n",
    "# import libaries\n",
    "import pandas as pd\n",
    "from src.config import Config\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# loading data_forsale_new.csv into df_main\n",
    "df_main = pd.read_csv(\"./data/data_forsale_new.csv\")\n",
    "\n",
    "# finding nan values \n",
    "\n",
    "# for x in df_main.columns:\n",
    "#    print(x, Config.expand_dataframe(df_main[df_main[x].isna() == True].shape))\n",
    "\n",
    "# # filling nan values \n",
    "# Config.fill_nan(df=df_main, column=\"plot_area\")\n",
    "# Config.expand_dataframe(df_main[df_main[\"plot_area\"].isna() == True])\n",
    "\n",
    "# feature selection\n",
    "## correlation\n",
    "\n",
    "display(df_main.columns)\n",
    "\n",
    "Config.expand_display(x=df_main[[\"price\", \"price_per_sqmeter\", \"plot_area\", \"habitable_surface\", \n",
    "         \"land_surface\", \"bedroom_count\", \"room_count\"]].corr()[\"price\"])\n",
    "\n",
    "\n",
    "df_set_one = df_main[[\"price\", \"bedroom_count\", \"room_count\", \"habitable_surface\"]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the calculated correlation value above, we can pick 3 variables that have an adequately significant correlation with variable **price**:\n",
    "1. **habitable_surface** : 0.344623\n",
    "2. **room_count**  : 0.376763\n",
    "3. **bedroom_count** : 0.382781"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "359000.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#################################################### LOAD AND PREPROCESS DATA ####################################################\n",
    "\n",
    "# Import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src.config import Config, ModelConfig\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "\n",
    "# Loading data_forsale_new.csv into df_main\n",
    "df_set_one = ModelConfig.load_data(filepath='./data/data_forsale_new.csv', file_type=\"csv\", usecols=[\"price\", \"room_count\", \"bedroom_count\", \"habitable_surface\"])\n",
    "\n",
    "# cheeck df_set_one\n",
    "Config.expand_display(df_set_one[\"price\"].median())\n",
    "\n",
    "# fill missing values\n",
    "# Config.fill_nan(df=df_set_one, column=[\"habitable_surface\", \"bedroom_count\", \"room_count\"])\n",
    "\n",
    "# Extract the features (X) and target (y)\n",
    "X, y = ModelConfig.feature_target_config(df=df_set_one)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "\n",
    "# Define column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num_imputer', KNNImputer(n_neighbors=5), ['room_count', \"bedroom_count\", \"habitable_surface\" ]),\n",
    "        ('poly_features', ModelConfig.poly_features_config(degree=5), [\"price\", \"room_count\", \"bedroom_count\", \"habitable_surface\"]),\n",
    "        ('std_scaler', StandardScaler(with_mean=False, with_std=True), [\"price\", \"room_count\", \"bedroom_count\", \"habitable_surface\"])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################### SETTING UP PIPELINE PARTS, INITIALIZING MODEL PIPELINE ####################################################\n",
    "from src.config import Config, ModelConfig\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "# Create the XGBRegressor with specified parameters\n",
    "xgb_reg = ModelConfig.XGBREGRConfig()\n",
    "\n",
    "# Create the model_pipeline with preprocessing and XGBoost regression\n",
    "model_pipeline = make_pipeline(preprocessor, xgb_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################### HYPERPARAMETER TUNING USING GRIDSEARCHCV ####################################################\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_absolute_percentage_error, explained_variance_score\n",
    "\n",
    "# Hyperparameter tuning using ModelConfig.XGB_ParamGrid\n",
    "param_grid = ModelConfig.XGB_ParamGrid\n",
    "\n",
    "# Specify the metric to use for refitting (explained variance score in this case)\n",
    "refit_metric = 'explained_variance'\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = ModelConfig.XGBGridSearchCV(estimator=model_pipeline, param=param_grid, cv_fold=5, scoring={\n",
    "    'explained_variance': 'explained_variance',\n",
    "    'mape': make_scorer(mean_absolute_percentage_error),\n",
    "    'r2': 'r2'\n",
    "}, refit=refit_metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 39690000 candidates, totalling 198450000 fits\n"
     ]
    }
   ],
   "source": [
    "#################################################### FIT AND TEST MODEL, SORT INDICES  AND EVALUATE THE MODEL ####################################################\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "# Fit the model_pipeline on the training data and generate predictions on the test data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the best model\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Sort the test data points based on feature values\n",
    "sort_indices = np.argsort(X_test[:, 0])\n",
    "X_test_sorted = X_test[sort_indices]\n",
    "y_test_sorted = y_test[sort_indices]\n",
    "y_pred_sorted = y_pred[sort_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################### EVALUATE THE MODEL ####################################################\n",
    "from src.model_config import ModelConfig\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, explained_variance_score\n",
    "\n",
    "# RMSE (Root Mean Squared Error)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "# MAPE (Mean Absolute Percentage Error)\n",
    "mape_score = ModelConfig.MAPE(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## subsetting df_main\n",
    "\n",
    "df_set_one = df_main[[\"price\", \"bedroom_count\", \"room_count\", \"habitable_surface\"]]\n",
    "\n",
    "Config.expand_display(x=df_set_one.describe())\n",
    "Config.expand_display(pd.DataFrame(df_set_one.median()))\n",
    "\n",
    "## removing extreme values\n",
    "df_set_one.drop(df_set_one[df_set_one[\"price\"] < 35000].index, inplace=True) \n",
    "                                                # 35000 is just an arbitrary number\n",
    "df_set_one.drop(df_set_one[df_set_one[\"price\"] > 5500000].index, inplace=True) \n",
    "                                                # 5500000 is just an arbitrary number\n",
    "\n",
    "## see the correlation if it is improving?\n",
    "Config.expand_display(df_set_one.corr()) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
